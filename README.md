# üéµ Music Performance Audio-Visual Question Answering Requires Specialized Multimodal Designs

Welcome to the companion repository for our position paper on Music Performance Audio-Visual Question Answering (Music AVQA). This repo curates the datasets, benchmark results, and seminal methods surveyed in the paper, and it will be continuously updated as the field evolves. Whether you are reproducing baselines, exploring bias-reduced or robustness-focused splits, or designing new music-specific architectures, you will find ready-to-use links, code pointers, and concise summaries here. We hope this living resource accelerates research on rich, densely layered audio-visual reasoning in musical contexts and sparks fresh ideas for multimodal music understanding.

## Content

- [Background](#background)
- [Structure](#structure)
- [Datasets](#datasets)
- [Methods](#methods)
- [Analysis](#analysis)

## Structure

Below is a visualized view of the repository.

```text
.
‚îú‚îÄ methods/
‚îÇ   ‚îú‚îÄ Amuse.pdf
‚îÇ   ‚îî‚îÄ ‚Ä¶ (more papers)
‚îú‚îÄ figs/
‚îî‚îÄ README.md
```

## Background

Music‚Äêperformance scenes span solo performances, ensemble of the same instrument, ensemble of different instruments, and culture-specific groups. Within this setting, Music AVQA questions generally fall into five categories, detailed in the table below.
The models listed in Section [Methods](#methods) are commonly evaluated on these question types, while ongoing work must still overcome dense-signal, hierarchical-temporal, and cross-modal challenges using the three main music AVQA benchmarks summarised in [Datasets](#datasets).

| Question Type | Description                                                                                | Example                                                         |
| ------------- | ------------------------------------------------------------------------------------------ | --------------------------------------------------------------- |
| Existential   | Determine whether an audible event corresponds to a visible object or action in the scene. | ‚ÄúIs the sound coming from the piano shown in the video?‚Äù        |
| Counting      | Estimate the number of audio-visual elements that meet a specified condition.              | ‚ÄúHow many instruments are playing simultaneously?‚Äù              |
| Location      | Identify the spatial position of the sound source within the visual frame.                 | ‚ÄúWhere is the first instrument that starts playing?‚Äù            |
| Comparative   | Compare an attribute of two or more audio-visual elements.                                 | ‚ÄúIs the violin on the left louder than the cello on the right?‚Äù |
| Temporal      | Reason about the order or timing of events across modalities.                              | ‚ÄúWhich instrument plays right after the drum beat?‚Äù             |

## Datasets

The table below tracks the progression of the MUSIC-AVQA datasets from the 2022 debut to later versions that reduce bias and enhance robustness. It includes direct download links, reference papers and concise descriptions of each release.

| Year | Dataset                                                                                                                                          | Paper Title                                                                                                                                                                                                                                     | Description                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |
| ---- | ------------------------------------------------------------------------------------------------------------------------------------------------ | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| 2022 | [Music-AVQA](https://drive.google.com/drive/folders/1WAryZZE0srLIZG8VHl22uZ3tpbGHtsrQ)                                                           | [Learning to Answer Questions in Dynamic Audio-Visual Scenarios](https://openaccess.thecvf.com/content/CVPR2022/papers/Li_Learning_To_Answer_Questions_in_Dynamic_Audio-Visual_Scenarios_CVPR_2022_paper.pdf)                                   | The MUSIC-AVQA dataset represents a significant contribution to audio-visual question answering research, comprising 9,288 videos with over 150 hours of musical performances covering 22 instruments, generating 45,867 question-answer pairs. The dataset is randomly split into training, validation, and testing sets with 32,087, 4,595, and 9,185 QA pairs respectively, spanning 33 question templates across 9 question types including existential, location, counting, comparative, and temporal questions.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |
| 2023 | [Music-AVQA-v2.0](https://www.dropbox.com/scl/fi/yfrrsuds95tmtgxirxy6e/collect_new_set.rar?rlkey=izt6ldb1czlpnhy8y6masi2jm&e=1&st=8o3m8ds9&dl=0) | [Tackling Data Bias in MUSIC-AVQA: Crafting a Balanced Dataset for Unbiased Question-Answering](https://openaccess.thecvf.com/content/WACV2024/papers/Liu_Tackling_Data_Bias_in_MUSIC-AVQA_Crafting_a_Balanced_Dataset_for_WACV_2024_paper.pdf) | The MUSIC-AVQA v2.0 dataset builds upon the original MUSIC-AVQA by addressing data bias issues, comprising 10,518 videos (9,288 from the original plus 1,230 new videos) with musical performances covering 22 instruments, generating approximately 54,000 question-answer pairs. The balanced dataset splits into training, validation, and testing sets with 36,700, 5,250, and 10,819 QA pairs respectively, spanning 33 question templates across 9 question types. The authors specifically balance 15 biased templates by ensuring no dominant answers exceed 60\% for binary questions or 50\% for multi-class questions, particularly enhancing representation of underrepresented answers in existential, counting, temporal, location, and comparative question categories.                                                                                                                                                                                                                                              |
| 2024 | [Music-AVQA-R](https://github.com/GeWu-Lab/MUSIC-AVQA/tree/main/data/json_update)                                                                | [Look, Listen, and Answer: Overcoming Biases for Audio-Visual Question Answering](https://proceedings.neurips.cc/paper_files/paper/2024/file/123a18dfd821c8b440f42a00a27648d6-Paper-Conference.pdf)                                             | The MUSIC-AVQA-R dataset proposed in this paper is an extension of MUSIC-AVQA specifically designed to evaluate the robustness of audio-visual question answering models. It expands the original test set through a human-machine collaboration mechanism that rephrases each question 25 times, increasing the number of questions from 9,129 to 211,572, and introduces distribution shifts to categorize questions into head (common) and tail (rare) samples. Compared to the original dataset, MUSIC-AVQA-R features a vocabulary size of 465 (five times larger than MUSIC-AVQA), provides more diverse question formulations while preserving inherent biases in the training and validation sets, and offers three evaluation metrics‚Äîhead accuracy, tail accuracy, and overall accuracy‚Äîenabling researchers to assess model performance in both in-distribution and out-of-distribution scenarios, making it the first dataset specifically designed for robustness evaluation in audio-visual question answering tasks. |

## Methods

The table below summarizes key milestone models in Visual and Audio-Visual QA, listing each method‚Äôs related paper, publication venue and any available code for quick experimentation.

| Year | Method         | Paper Title                                                                                                                                                                                                                                                     | Conference/Journal | Code                                                                |
| ---- | -------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------ | ------------------------------------------------------------------- |
| 2015 | GRU            | [VQA: Visual Question Answering](https://openaccess.thecvf.com/content_iccv_2015/papers/Antol_VQA_Visual_Question_ICCV_2015_paper.pdf)                                                                                                                          | ICCV 15'           | -                                                                   |
| 2019 | AVSD           | [A Simple Baseline for Audio-Visual Scene-Aware Dialog](https://openaccess.thecvf.com/content_CVPR_2019/papers/Schwartz_A_Simple_Baseline_for_Audio-Visual_Scene-Aware_Dialog_CVPR_2019_paper.pdf)                                                              | CVPR 19'           | [Github](https://github.com/idansc/simple-avsd)                     |
| 2019 | MCAN           | [Deep Modular Co-Attention Networks for Visual Question Answering](https://openaccess.thecvf.com/content_CVPR_2019/papers/Yu_Deep_Modular_Co-Attention_Networks_for_Visual_Question_Answering_CVPR_2019_paper.pdf)                                              | CVPR 19'           | [Github](https://github.com/MILVLG/mcan-vqa)                        |
| 2019 | PSAC           | [Beyond RNNs: Positional Self-Attention with Co-Attention for Video Question Answering](https://ojs.aaai.org/index.php/AAAI/article/download/4887/4760)                                                                                                         | AAAI 19'           | [Github](https://github.com/lixiangpengcs/PSAC)                     |
| 2020 | HCRN           | [Hierarchical Conditional Relation Networks for Video Question Answering](https://openaccess.thecvf.com/content_CVPR_2020/papers/Le_Hierarchical_Conditional_Relation_Networks_for_Video_Question_Answering_CVPR_2020_paper.pdf)                                | CVPR 20'           | [Github](https://github.com/thaolmk54/hcrn-videoqa)                 |
| 2021 | LAViT          | [Pano-AVQA: Grounded Audio-Visual Question Answering on 360‚ó¶ Videos](https://hs-yn.github.io/assets/pdf/2021iccv_panoavqa.pdf)                                                                                                                                  | ICCV 21'           | [Github](https://github.com/HS-YN/PanoAVQA)                         |
| 2022 | AVST           | [Learning to Answer Questions in Dynamic Audio-Visual Scenarios](https://openaccess.thecvf.com/content/CVPR2022/papers/Li_Learning_To_Answer_Questions_in_Dynamic_Audio-Visual_Scenarios_CVPR_2022_paper.pdf)                                                   | CVPR 22'           | [Github](https://github.com/GeWu-Lab/MUSIC-AVQA)                    |
| 2023 | ChatBridge     | [ChatBridge: Bridging Modalities with Large Language Model as a Language Catalyst](https://arxiv.org/pdf/2305.16103)                                                                                                                                            | CoRR 23'           | [Github](https://github.com/joez17/ChatBridge)                      |
| 2023 | CIGN           | [Class-Incremental Grouping Network for Continual Audio-Visual Learning](https://openaccess.thecvf.com/content/ICCV2023/papers/Mo_Class-Incremental_Grouping_Network_for_Continual_Audio-Visual_Learning_ICCV_2023_paper.pdf)                                   | ICCV 23'           | [Github](https://github.com/stoneMo/CIGN)                           |
| 2023 | COCA           | [COCA: COllaborative CAusal Regularization for Audio-Visual Question Answering](https://ojs.aaai.org/index.php/AAAI/article/download/26527/26299)                                                                                                               | AAAI 23'           | -                                                                   |
| 2023 | CONVLSTM       | [Temporal Reasoning via Audio Question Answering](https://aclanthology.org/2023.findings-emnlp.630.pdf)                                                                                                                                                         | EMNLP 23'          | [Github](https://github.com/Bravo5542/TJSTG)                        |
| 2023 | FCNLSTM        | [Temporal Reasoning via Audio Question Answering](https://aclanthology.org/2023.findings-emnlp.630.pdf)                                                                                                                                                         | EMNLP 23'          | [Github](https://github.com/Bravo5542/TJSTG)                        |
| 2023 | DCL            | [Disentangled Counterfactual Learning for Physical Audiovisual Commonsense Reasoning](https://proceedings.neurips.cc/paper_files/paper/2023/file/29571f8fda54fe93631c41aad4215abc-Paper-Conference.pdf)                                                         | NIPS 23'           | [Github](https://github.com/Andy20178/DCL)                          |
| 2023 | DG-SCT         | [Cross-modal Prompts: Adapting Large Pre-trained Models for Audio-Visual Downstream Tasks](https://proceedings.neurips.cc/paper_files/paper/2023/file/af01716e08073368a7c8a62be46dba17-Paper-Conference.pdf)                                                    | NIPS 23'           | [Github](https://github.com/haoyi-duan/DG-SCT)                      |
| 2023 | LAVisH         | [Vision Transformers are Parameter-Efficient Audio-Visual Learners](https://openaccess.thecvf.com/content/CVPR2023/papers/Lin_Vision_Transformers_Are_Parameter-Efficient_Audio-Visual_Learners_CVPR_2023_paper.pdf)                                            | CVPR 23'           | [Github](https://github.com/GenjiB/LAVISH)                          |
| 2023 | LSTTA          | [Parameter-Efficient Transfer Learning for Audio-Visual-Language Tasks](https://arxiv.org/pdf/2308.14274)                                                                                                                                                       | MM 23'             | -                                                                   |
| 2023 | PSTP-Net       | [Progressive Spatio-temporal Perception for Audio-Visual Question Answering](https://arxiv.org/pdf/2308.05421)                                                                                                                                                  | MM 23'             | [Github](https://github.com/GeWu-Lab/PSTP-Net)                      |
| 2023 | VAST           | [VAST: A Vision-Audio-Subtitle-Text Omni-Modality Foundation Model and Dataset](https://arxiv.org/pdf/2305.18500)                                                                                                                                               | NIPS 23'           | [Github](https://github.com/TXH-mercury/VAST)                       |
| 2024 | Amuse          | [Learning Musical Representations for Music Performance Question Answering](https://aclanthology.org/2024.findings-emnlp.159.pdf)                                                                                                                               | EMNLP 24'          | [Github](https://github.com/xid32/Amuse?tab=readme-ov-file)         |
| 2024 | Audio Flamingo | [Audio Flamingo: a novel audio language model with few-shot learning and dialogue abilities](https://arxiv.org/pdf/2402.01831)                                                                                                                                  | ICML 24'           | [Github](https://github.com/NVIDIA/audio-flamingo)                  |
| 2024 | AVMoE          | [Mixture of Experts for Audio-Visual Learning](https://proceedings.neurips.cc/paper_files/paper/2024/file/009729d26288b9a8826023692a876107-Paper-Conference.pdf)                                                                                                | NIPS 24'           | [Github](https://github.com/yingchengy/AVMOE)                       |
| 2024 | AVSiam         | [Siamese Vision Transformers are Scalable Audio-visual Learners](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/02220.pdf)                                                                                                                            | ECCV 24'           | [Github](https://github.com/GenjiB/AVSiam)                          |
| 2024 | CAT            | [CAT: Enhancing Multimodal Large Language Model to Answer Questions in Dynamic Audio-Visual Scenarios](https://arxiv.org/pdf/2403.04640)                                                                                                                        | ECCV 24'           | [Github](https://github.com/rikeilong/Bay-CAT)                      |
| 2024 | CrossMAE       | [CrossMAE: Cross-Modality Masked Autoencoders for Region-Aware Audio-Visual Pre-Training](https://openaccess.thecvf.com/content/CVPR2024/papers/Guo_CrossMAE_Cross-Modality_Masked_Autoencoders_for_Region-Aware_Audio-Visual_Pre-Training_CVPR_2024_paper.pdf) | CVPR 24'           | [Github](https://github.com/TonyLianLong/CrossMAE)                  |
| 2024 | EEMC           | [Ref-AVS: Refer and Segment Objects in Audio-Visual Scenes](https://arxiv.org/pdf/2407.10957)                                                                                                                                                                   | ECCV 24'           | [Github](https://github.com/GeWu-Lab/Ref-AVS)                       |
| 2024 | GPT-4o         | [GPT-4o System Card](https://arxiv.org/pdf/2410.21276)                                                                                                                                                                                                          | -                  | [Github](https://github.com/marketplace/models/azure-openai/gpt-4o) |
| 2024 | LAST-Att       | [Tackling Data Bias in MUSIC-AVQA: Crafting a Balanced Dataset for Unbiased](https://openaccess.thecvf.com/content/WACV2024/papers/Liu_Tackling_Data_Bias_in_MUSIC-AVQA_Crafting_a_Balanced_Dataset_for_WACV_2024_paper.pdf)                                    | WACV 24'           | [Github](https://github.com/DragonLiu1995/MUSIC-AVQA-v2.0)          |
| 2024 | MAVEN          | [FortisAVQA and MAVEN: a Benchmark Dataset and Debiasing Framework for Robust Multimodal Reasoning](https://arxiv.org/pdf/2504.00487)                                                                                                                           | NIPS 24'           | [Github](https://github.com/reml-group/fortisavqa)                  |
| 2024 | MCCD           | [Look, Listen, and Answer: Overcoming Biases for Audio-Visual Question Answering](https://proceedings.neurips.cc/paper_files/paper/2024/file/123a18dfd821c8b440f42a00a27648d6-Paper-Conference.pdf)                                                             | NIPS 24'           | [Github](https://github.com/reml-group/MUSIC-AVQA-R)                |
| 2024 | Meerkat        | [Meerkat: Audio-Visual Large Language Model for Grounding in Space and Time](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/08071.pdf)                                                                                                                | ECCV 24'           | [Github](https://github.com/schowdhury671/meerkat)                  |
| 2024 | OGM            | [On-the-fly Modulation for Balanced Multimodal Learning](https://arxiv.org/pdf/2410.11582)                                                                                                                                                                      | T-PAMI 24'         | [Github](https://github.com/GeWu-Lab/BML_TPAMI2024)                 |
| 2024 | OneLLM         | [OneLLM: One Framework to Align All Modalities with Language](https://openaccess.thecvf.com/content/CVPR2024/papers/Han_OneLLM_One_Framework_to_Align_All_Modalities_with_Language_CVPR_2024_paper.pdf)                                                         | CVPR 24'           | [Github](https://github.com/csuhan/OneLLM)                          |
| 2024 | OPM            | [On-the-fly Modulation for Balanced Multimodal Learning](https://arxiv.org/pdf/2410.11582)                                                                                                                                                                      | T-PAMI 24'         | [Github](https://github.com/GeWu-Lab/BML_TPAMI2024)                 |
| 2024 | QaP            | [Querying as Prompt: Parameter-Efficient Learning for Multimodal Language Model](https://openaccess.thecvf.com/content/CVPR2024/papers/Liang_Querying_as_Prompt_Parameter-Efficient_Learning_for_Multimodal_Language_Model_CVPR_2024_paper.pdf)                 | CVPR 24'           | [Github](https://github.com/Rainlt/QaP/)                            |
| 2024 | RefAtomNet     | [Referring Atomic Video Action Recognition](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/02873.pdf)                                                                                                                                                 | ECCV 24'           | -                                                                   |
| 2024 | VALOR          | [VALOR: Vision-Audio-Language Omni-Perception Pretraining Model and Dataset](https://arxiv.org/pdf/2304.08345)                                                                                                                                                  | T-PAMI 24'         | [Github](https://github.com/TXH-mercury/VALOR)                      |
| 2024 | VideoLLaMA-2   | [VideoLLaMA 2 Advancing Spatial-Temporal Modeling and Audio Understanding in Video-LLMs](https://arxiv.org/pdf/2406.07476)                                                                                                                                      | -                  | [Github](https://github.com/DAMO-NLP-SG/VideoLLaMA2)                |
| 2024 | VITA           | [VITA: Towards Open-Source Interactive Omni Multimodal LLM](https://arxiv.org/pdf/2408.05211)                                                                                                                                                                   | CoRR 24'           | [Github](https://github.com/VITA-MLLM/VITA)                         |
| 2025 | Qwen2.5-VL     | [Qwen2.5-VL Technical Report](https://arxiv.org/pdf/2502.13923)                                                                                                                                                                                                 | -                  | [Github](https://github.com/QwenLM/Qwen2.5-VL)                      |

## Analysis

Figures (a) and (b) below visualize the average performance of popular methods across the five canonical Music-AVQA question types on the original Music-AVQA and Music-AVQA-R, respectively. Methods without explicit spatial-temporal design are shaded in blue, while methods that incorporate dedicated spatial-temporal modules are shown in green and purple. A clear accuracy gap illustrates how spatial-temporal reasoning benefits both in-distribution and out-of-distribution scenarios. Full numerical results and an extended discussion can be found in the main paper.
| (a) Methods on Music-AVQA. | (b) Methods on Muisc-AVQA-R. |
|---------------|---------------|
| ![Fig-(a)](https://i.imgur.com/GJByBtw.png) | ![Fig-(b)](https://imgur.com/PLzjd2B.png) |

```

```
